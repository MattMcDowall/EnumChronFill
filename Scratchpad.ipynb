{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186fdbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os.path import exists as file_exists\n",
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "import re\n",
    "import requests\n",
    "from time import sleep\n",
    "import xmltodict\n",
    "import Credentials      # Get API keys, etc.\n",
    "\n",
    "apikey = Credentials.prod_api\n",
    "baseurl = 'https://api-na.hosted.exlibrisgroup.com'\n",
    "item_query = '/almaws/v1/bibs/{mms_id}/holdings/{holding_id}/items/{item_pid}?apikey={apikey}'\n",
    "\n",
    "exported_csv = \"FullItemList.csv\"\n",
    "filled_csv = \"FilledEnumChron.csv\"\n",
    "err_log_txt = \"log.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52ac8a",
   "metadata": {},
   "source": [
    "### Run this cell if you want to use the Limited-list CSV\n",
    "exported_csv = \"LimitedList.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60e89d",
   "metadata": {},
   "source": [
    "#### Create original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0706ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on how the file was exported, column names may or may have either spaces or underscores\n",
    "df = pd.read_csv(exported_csv, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918ffd4",
   "metadata": {},
   "source": [
    "#### Clean up, tweak & format df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7adaf483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces from column names\n",
    "df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "# Rename certain columns\n",
    "df = df.rename(columns={'Permanent_Location': 'Location', 'Item_Policy': 'Policy', 'Material_Type': 'Material'})\n",
    "# Strip leading/trailing space from Description\n",
    "df.Description = df.Description.str.strip()\n",
    "# Collapse multiple spaces within the Description\n",
    "df.Description.replace(' +', ' ', regex=True, inplace=True)\n",
    "\n",
    "# Exclude special casesâ€”GovDocs with weird numbering\n",
    "df = df[~((df['Material'].isin(['Issue','Microform','Other'])) & (df['Location'].isin(['gdmo','gdrf','gdrfi','govdo'])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c337513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for the Enum/Chron fields\n",
    "EC_fields = ['Enum_A', 'Enum_B', 'Enum_C', 'Chron_I', 'Chron_J']\n",
    "df[EC_fields] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6bfde9",
   "metadata": {},
   "source": [
    "#### The function for getting info from Description to Enum/Chron fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "983e780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_and_extract(regex, these_fields):\n",
    "    exp = re.compile(regex, re.IGNORECASE)\n",
    "    for i, f in enumerate(these_fields):\n",
    "        df[f] = df['Description'].str.extract(exp, expand=True)[i].fillna(df[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c90144",
   "metadata": {},
   "source": [
    "#### Set some common expressions that can be used in a modular way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1d911606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single volume/book - CAPTURE\n",
    "vvvRE = r'(?:v|bk)\\. ?(\\d+[a-z]?)'\n",
    "# Volume(s)/book(s) - CAPTURE\n",
    "vvv_vvRE = vvvRE[:-1] + r'(?:[\\&\\-]\\d+)?)'\n",
    "# Index/Supp/etc\n",
    "iiiiRE = r'(?:abstracts?|addendum|brief|Directory|exec(?:utive)? summ(?:ary)?|guide|handbook|(?:author |cum |master |subj )?Index(?:es)?|revisions?|spec(?:ial(?:edition|issue|rep|report)?)?|Suppl?\\.?(?: \\d+)? ?)|title sheet|updates?'\n",
    "# Month/season\n",
    "mmmRE = r'(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?|Spr(?:ing)?|Sum(?:mer)?|Fall?|Aut(?:umn)?|Win(?:ter)?)'\n",
    "# Month + date(s)\n",
    "mmm_ddRE = mmmRE + r'(?: \\d{1,2}(?:[\\-\\/]\\d{1,2})?)?'\n",
    "# 4-digit year/range of years (post-18th-century)\n",
    "yyyyRE = r'(?:1[89]|20)\\d{2}'\n",
    "# 4-digit year leading to a range\n",
    "yyyy_yyRE = r'(?:1[89]|20)\\d{2}(?:-\\d{2}|-\\d{4})?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0daf1",
   "metadata": {},
   "source": [
    "#### Call it repeatedly for each regex that you come up with:\n",
    "**[Test your Regex here](https://regex101.com/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25e17b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just volume(s) & nothing else\n",
    "fill_and_extract(r'^' + vvv_vvRE + '$',\n",
    "                 ['Enum_A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ced5fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume + issue\n",
    "fill_and_extract(r'^' + vvvRE + '[ \\/]no\\. ?(\\d+)$',\n",
    "                 ['Enum_A', 'Enum_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db272298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vol + issue + date + year\n",
    "fill_and_extract(r'^' + vvvRE + '[ \\/]no\\. ?(\\d+) (' + mmm_ddRE + r'),? (' + yyyyRE + ')$',\n",
    "                 ['Enum_A', 'Enum_B', 'Chron_J', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "50e7e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume(s) + \"Index\"\n",
    "fill_and_extract(r'^' + vvv_vvRE + ' (' + iiiiRE + r')$',\n",
    "                 ['Enum_A', 'Enum_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3f88af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume + year(s)\n",
    "fill_and_extract(r'^' + vvv_vvRE + ' +\\(?(' + yyyy_yyRE + r')\\)?$',\n",
    "                 ['Enum_A', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aee6e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just issue & nothing else\n",
    "fill_and_extract(r'^no\\. ?(\\d+)$',\n",
    "                 ['Enum_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7ef8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue + year(s)\n",
    "fill_and_extract(r'^no\\. ?(\\d+) (' + yyyy_yyRE + r')$',\n",
    "                 ['Enum_B', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "26e86f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just part(s) & nothing else\n",
    "fill_and_extract(r'^pt\\. ?(\\d+[a-z]?(?:[\\&\\-]\\d+)?)$',\n",
    "                 ['Enum_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c02c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just year/range of years\n",
    "fill_and_extract(r'^(' + yyyy_yyRE + r')$',\n",
    "                 ['Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dff7e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year(s) + volume(s)\n",
    "fill_and_extract(r'^(' + yyyy_yyRE + r') ' + vvv_vvRE + '$',\n",
    "                 ['Chron_I', 'Enum_A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "40830773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year(s) + part(s)\n",
    "fill_and_extract(r'^(' + yyyy_yyRE + r') pt\\. ?(\\d+(?:[\\-\\/]\\d+)?)$',\n",
    "                 ['Chron_I', 'Enum_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fcadaeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year(s) + \"Index\"\n",
    "fill_and_extract(r'^(' + yyyy_yyRE + r') (' + iiiiRE + r')$',\n",
    "                 ['Chron_I', 'Enum_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "650400e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year + month/season/date\n",
    "fill_and_extract(r'^(' + yyyyRE + r') (' + mmm_ddRE + r')$',\n",
    "                 ['Chron_I', 'Chron_J'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b1a765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of dates within one calendar year\n",
    "fill_and_extract(r'^(' + mmm_ddRE + r'[\\-\\/]' + mmm_ddRE + '), (' + yyyyRE + ')$',\n",
    "                 ['Chron_J', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f5773bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date + year\n",
    "fill_and_extract(r'^(' + mmm_ddRE + '),? (' + yyyyRE + ')$',\n",
    "                 ['Chron_J', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af8e22",
   "metadata": {},
   "source": [
    "#### Then once all those replacements are done, pull filled-in records out to a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "184ebb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to hold JUST records that get filled\n",
    "filled = pd.DataFrame()\n",
    "# Populate the new dataframe with any records that now have at least one Enum/Chron field filled\n",
    "filled = df.dropna(subset=EC_fields, thresh=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a955610",
   "metadata": {},
   "source": [
    "#### Then apply the changes via the API and log filled items to the Filled CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca0ea802",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = len(filled)\n",
    "c = 0\n",
    "needs_header=not file_exists(filled_csv) # Apparently we're creating the file, so it needs a header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this bit to see what got \"filled\" before hitting the API\n",
    "\n",
    "for index, row in filled.fillna('').iterrows():\n",
    "    c += 1\n",
    "    print(c, ' / '.join([row['MMS_ID'], row['Holdings_ID'], row['Item_ID']]),\n",
    "          str(row['Description']),\n",
    "          ' | '.join(x or '' for x in [row['Enum_A'], row['Enum_B'], row['Enum_C'], row['Chron_I'], row['Chron_J']]),\n",
    "          sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da309b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1671cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d5538c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c5eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5092f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(err_log_txt, 'a') as err_log:\n",
    "    for index, row in filled.fillna('').iterrows():\n",
    "        c += 1\n",
    "        r = requests.get(''.join([baseurl,\n",
    "                                  item_query.format(mms_id=str(row['MMS_ID']),\n",
    "                                                    holding_id=str(row['Holdings_ID']),\n",
    "                                                    item_pid=str(row['Item_ID']),\n",
    "                                                    apikey=apikey)]))\n",
    "        rdict = xmltodict.parse(r.text)\n",
    "        if r.status_code == 429:  # Too many requests--daily limit\n",
    "            print()\n",
    "            print('Reached API request limit for today. Stopping execution.')\n",
    "            print()            \n",
    "            ## Drop this record & everything after from \"filled\"\n",
    "            filled = filled.iloc[:c-1]\n",
    "            break\n",
    "        if r.status_code != 200:\n",
    "            e = xmltodict.parse(r._content)\n",
    "            # Log the error\n",
    "            print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                  ' Error FETCHING item ', row['Item_ID'], ': (', r.status_code, ') ',\n",
    "                  e['web_service_result']['errorList']['error']['errorMessage'],\n",
    "                  sep='',\n",
    "                  file=err_log)\n",
    "            # Remove this item from the \"filled\" df\n",
    "            filled = filled.drop([index])\n",
    "            continue\n",
    "        if (c % (records/100) < 1):\n",
    "            print(int(100*c/records), '% complete', sep='')#, end='\\r')\n",
    "            sleep(5)\n",
    "            \n",
    "        # Merge derived values into the retrieved data (rdict)\n",
    "        rdict['item']['item_data']['enumeration_a'] = str(row['Enum_A'])\n",
    "        rdict['item']['item_data']['enumeration_b'] = str(row['Enum_B'])\n",
    "        rdict['item']['item_data']['enumeration_c'] = str(row['Enum_C'])\n",
    "        rdict['item']['item_data']['chronology_i'] = str(row['Chron_I'])\n",
    "        rdict['item']['item_data']['chronology_j'] = str(row['Chron_J'])\n",
    "        # Set an internal note, if there's an empty one available\n",
    "        if ('Enum/Chron derived from Description' not in rdict['item']['item_data'].values()):\n",
    "            if (not rdict['item']['item_data']['internal_note_1']):\n",
    "                rdict['item']['item_data']['internal_note_1'] = 'Enum/Chron derived from Description'\n",
    "            elif (not rdict['item']['item_data']['internal_note_2']):\n",
    "                rdict['item']['item_data']['internal_note_2'] = 'Enum/Chron derived from Description'\n",
    "            elif (not rdict['item']['item_data']['internal_note_3']):\n",
    "                rdict['item']['item_data']['internal_note_3'] = 'Enum/Chron derived from Description'\n",
    "            else: # Nbd, just log it\n",
    "                print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                      ' No internal note available for item MMS ID ',\n",
    "                      str(row['MMS_ID']), sep=\"\", file=err_log)\n",
    " \n",
    "        # Push the altered record back into Alma\n",
    "        pxml = xmltodict.unparse(rdict)\n",
    "        p = requests.put(''.join([baseurl,\n",
    "                                  item_query.format(mms_id=row['MMS_ID'],\n",
    "                                                    holding_id=row['Holdings_ID'],\n",
    "                                                    item_pid=row['Item_ID'],\n",
    "                                                    apikey=apikey)]),\n",
    "                         data=pxml.encode('utf-8'), headers={'Content-Type': 'application/xml'})\n",
    "        if r.status_code == 429:  # Too many requests--daily limit\n",
    "            print()\n",
    "            print('Reached API request limit for today. Stopping execution.')\n",
    "            print()            \n",
    "            ## Drop this record & everything after from \"filled\"\n",
    "            filled = filled.iloc[:c-1]\n",
    "            break\n",
    "        if p.status_code != 200:\n",
    "            e = xmltodict.parse(p._content)\n",
    "            # Log the error\n",
    "            print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), ' Error UPDATING item ', row['Item_ID'], ': (', p.status_code, ') ',\n",
    "                e['web_service_result']['errorList']['error']['errorMessage'],\n",
    "                sep='', file=err_log)\n",
    "            # Remove this item from the \"filled\" df\n",
    "            filled = filled.drop([index])\n",
    "            continue\n",
    "        print(c, ' / '.join([row['MMS_ID'], row['Holdings_ID'], row['Item_ID']]),\n",
    "              row['Description'],\n",
    "              ' | '.join(x or '' for x in [row['Enum_A'], row['Enum_B'], row['Chron_I'], row['Chron_J']]),\n",
    "              sep=\"\\t\")\n",
    "\n",
    "        # Log it to the CSV\n",
    "        #    Btw, the 'to_frame().T' transposes it, so it all goes in as a single comma-separated row\n",
    "        row.to_frame().T.to_csv(filled_csv, mode='a', index=False, header=needs_header)\n",
    "        needs_header = False # Henceforth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c1766",
   "metadata": {},
   "source": [
    "#### Purge filled rows from the original CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e16ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purge filled records from the original df\n",
    "df = df.loc[~df['Item_ID'].isin(filled['Item_ID'])]\n",
    "\n",
    "# Re-create the original CSV from that df\n",
    "df.to_csv(exported_csv, index=False) # By default, will overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e271555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f22e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3016a14",
   "metadata": {},
   "source": [
    "# View df & filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab226630",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "display(filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517d2ba",
   "metadata": {},
   "source": [
    "# <span style=\"color:#cc0000\">Undo</span> changes to some records!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6805ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "undo_csv=\"Undo.csv\"\n",
    "df = pd.read_csv(undo_csv, converters={'Item_ID': str, 'Holdings_ID': str, 'MMS_ID': str, 'Item ID': str, 'Holdings ID': str, 'MMS ID': str, 'Enum_A': str, 'Enum_B': str, 'Chron_I': str, 'Chron_J': str})\n",
    "# Remove spaces from column names\n",
    "df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "# Rename certain columns\n",
    "df = df.rename(columns={'Permanent_Location': 'Location', 'Item_Policy': 'Policy', 'Material_Type': 'Material'})\n",
    "records = len(df)\n",
    "c = 0\n",
    "\n",
    "for index, row in df.fillna('').iterrows():\n",
    "    c += 1\n",
    "    \n",
    "    # Get the current record\n",
    "    print(c, ' / '.join((row['MMS_ID'], row['Holdings_ID'], row['Item_ID'])), str(row['Description']), sep=\"\\t\")\n",
    "    r = requests.get(''.join([baseurl,\n",
    "                              item_query.format(mms_id=str(row['MMS_ID']),\n",
    "                                                holding_id=str(row['Holdings_ID']),\n",
    "                                                item_pid=str(row['Item_ID']),\n",
    "                                                apikey=apikey)]))\n",
    "    rdict = xmltodict.parse(r.text)\n",
    "    if r.status_code != 200:\n",
    "        e = xmltodict.parse(r._content)\n",
    "        # Output the error\n",
    "        print('Error FETCHING item ', row['Item_ID'], ': (', r.status_code, ') ',\n",
    "            e['web_service_result']['errorList']['error']['errorMessage'],\n",
    "            sep='')\n",
    "        continue\n",
    "    if (c % (records/100) < 1):\n",
    "        print(int(100*c/records), '% complete', sep='')#, end='\\r')\n",
    "        sleep(5)\n",
    "\n",
    "    # Merge derived values into the retrieved data (rdict)\n",
    "    rdict['item']['item_data']['enumeration_a'] = \\\n",
    "        rdict['item']['item_data']['enumeration_b'] = \\\n",
    "        rdict['item']['item_data']['enumeration_c'] = \\\n",
    "        rdict['item']['item_data']['chronology_i'] = \\\n",
    "        rdict['item']['item_data']['chronology_j'] = None\n",
    "    # Set an internal note, if there's an empty one available\n",
    "    if (rdict['item']['item_data']['internal_note_1'] == 'Enum/Chron derived from Description'):\n",
    "        rdict['item']['item_data']['internal_note_1'] = None\n",
    "    if (rdict['item']['item_data']['internal_note_2'] == 'Enum/Chron derived from Description'):\n",
    "        rdict['item']['item_data']['internal_note_2'] = None\n",
    "    if (rdict['item']['item_data']['internal_note_3'] == 'Enum/Chron derived from Description'):\n",
    "        rdict['item']['item_data']['internal_note_3'] = None\n",
    "\n",
    "    # Push the altered record back into Alma\n",
    "    pxml = xmltodict.unparse(rdict)\n",
    "    p = requests.put(''.join([baseurl,\n",
    "                              item_query.format(mms_id=row['MMS_ID'],\n",
    "                                                holding_id=row['Holdings_ID'],\n",
    "                                                item_pid=row['Item_ID'],\n",
    "                                                apikey=apikey)]),\n",
    "                     data=pxml.encode('utf-8'), headers={'Content-Type': 'application/xml'})\n",
    "    if p.status_code != 200:\n",
    "        e = xmltodict.parse(p._content)\n",
    "        # Log the error\n",
    "        print('Error UPDATING item ', row['Item_ID'], ': (', p.status_code, ') ',\n",
    "            e['web_service_result']['errorList']['error']['errorMessage'],\n",
    "            sep='')\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb63aa",
   "metadata": {},
   "source": [
    "# Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4e97c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?:v|bk)\\. ?(\\d+[a-z]?(?:[\\&\\-]\\d+)?)\n",
      "(?:v|bk)\\. ?(\\d+[a-z]?(?:[\\&\\-]\\d+)?)\n"
     ]
    }
   ],
   "source": [
    "foo = vvvRE[:-1] + r'(?:[\\&\\-]\\d+)?)'\n",
    "print(foo)\n",
    "print(vvv_vvRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "5fef9b4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df[~((df['Material'] == 'Issue') & (df['Location'] == 'gdmo'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fde678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
