{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebc5f68",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Import modules, set variables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5e9e2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T18:44:32.577408Z",
     "start_time": "2023-05-08T18:44:31.101451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from os.path import exists as file_exists\n",
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "import re\n",
    "import requests\n",
    "from time import sleep\n",
    "import xmltodict\n",
    "import Credentials      # Get API keys, etc.\n",
    "\n",
    "apikey = Credentials.prod_api\n",
    "baseurl = 'https://api-na.hosted.exlibrisgroup.com'\n",
    "item_query = '/almaws/v1/bibs/{mms_id}/holdings/{holding_id}/items/{item_pid}?apikey={apikey}'\n",
    "\n",
    "exported_csv = \"FullItemList.csv\"\n",
    "filled_csv = \"FilledEnumChron.csv\"\n",
    "err_log_txt = \"log.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d46626",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read CSV into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de22be41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T13:18:01.511919Z",
     "start_time": "2023-04-04T13:18:01.301938Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Depending on how the file was exported, column names may or may have either spaces or underscores\n",
    "df = pd.read_csv(exported_csv, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0666089",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Clean up, tweak & format df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c02182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T13:19:51.769702Z",
     "start_time": "2023-04-04T13:19:51.615096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove spaces from column names\n",
    "df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "# Rename certain columns\n",
    "df = df.rename(columns={'Permanent_Location':'Location',\n",
    "                        'Item_Policy': 'Policy',\n",
    "                        'Material_Type': 'Material'})\n",
    "# Strip leading/trailing space from Description\n",
    "df.Description = df.Description.str.strip()\n",
    "# Collapse multiple spaces within the Description\n",
    "df.Description.replace(' +', ' ', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f635585d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T13:19:55.255866Z",
     "start_time": "2023-04-04T13:19:55.209847Z"
    },
    "hidden": true
   },
   "source": [
    "# Exclude special cases—GovDocs of certain material types, with weird numbering\n",
    "df = df[~((df['Material'].isin(['Issue','Microform','Other'])) & (df['Location'].isin(['gdmo','gdrf','gdrfi','govdo'])))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5763fa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Add empty columns for the Enum/Chron fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981414d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T13:22:48.716005Z",
     "start_time": "2023-04-04T13:22:48.665797Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EC_fields = ['Enum_A', 'Enum_B', 'Enum_C', 'Chron_I', 'Chron_J']\n",
    "df[EC_fields] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e3fcf",
   "metadata": {},
   "source": [
    "### The function for getting info from Description to Enum/Chron fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd79860",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-04T13:23:05.990985Z",
     "start_time": "2023-04-04T13:23:05.977021Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def fill_and_extract(regex, these_fields):\n",
    "    exp = re.compile(regex, re.IGNORECASE)\n",
    "    for i, f in enumerate(these_fields):\n",
    "        df[f] = df['Description'].str.extract(exp, expand=True)[i].fillna(df[f])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831cab2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Set some common expressions that can be used in a modular way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc1439",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Single volume/book - CAPTURE\n",
    "vvvRE = r'(?:v|bk)\\. ?(\\d+[a-z]?)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0a96ae",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Volume(s)/book(s) - CAPTURE\n",
    "vvv_vvRE = vvvRE[:-1] + r'(?:[\\&\\-]\\d+)?)'\n",
    "# Index/Supp/etc\n",
    "iiiiRE = r'(?:abstracts?|addendum|brief|Directory|exec(?:utive)? summ(?:ary)?|guide|handbook|(?:author |cum |master |subj )?Index(?:es)?|revisions?|spec(?:ial(?:edition|issue|rep|report)?)?|Suppl?\\.?(?: \\d+)? ?)|title sheet|updates?'\n",
    "# Month/season\n",
    "mmmRE = r'(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?|Spr(?:ing)?|Sum(?:mer)?|Fall?|Aut(?:umn)?|Win(?:ter)?)'\n",
    "# Month + date(s)\n",
    "mmm_ddRE = mmmRE + r'(?: \\d{1,2}(?:[\\-\\/]\\d{1,2})?)?'\n",
    "# 4-digit year/range of years (post–18th-century)\n",
    "yyyyRE = r'(?:1[89]|20)\\d{2}'\n",
    "# 4-digit year leading to a range\n",
    "#   E.g., 1976-89\n",
    "yyyy_yyRE = r'(?:1[89]|20)\\d{2}(?:-\\d{2}|-\\d{4})?'\n",
    "# 4-digit year leading to a range, possibly using a SLASH\n",
    "#   E.g., 1976/89\n",
    "#   Beware false positives\n",
    "#   Remember to replace the slash with a hyphen in the Chron_I\n",
    "yyyy_SyyRE = r'(?:1[89]|20)\\d{2}(?:[\\-\\/]\\d{2}|[\\-\\/]\\d{4})?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa3279",
   "metadata": {},
   "source": [
    "#### Call it repeatedly for each regex that you come up with:\n",
    "**[Test your Regex here](https://regex101.com/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b7323ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just volume(s) & nothing else\n",
    "fill_and_extract(r'^' + vvv_vvRE + '$',\n",
    "                 ['Enum_A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ad4793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume + issue\n",
    "fill_and_extract(r'^' + vvvRE + '[ \\/]no\\. ?(\\d+)$',\n",
    "                 ['Enum_A', 'Enum_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a46125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vol + issue + year\n",
    "fill_and_extract(r'^' + vvvRE + r'[ \\/]no\\. ?(\\d+) (' + yyyyRE + r')$',\n",
    "    ['Enum_A', 'Enum_B', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b77b3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vol + issue + date + year\n",
    "fill_and_extract(r'^' + vvvRE + '[ \\/]no\\. ?(\\d+) (' + mmm_ddRE + r'),? (' + yyyyRE + ')$',\n",
    "                 ['Enum_A', 'Enum_B', 'Chron_J', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c21d39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume(s) + \"Index\"\n",
    "fill_and_extract(r'^' + vvv_vvRE + ' (' + iiiiRE + r')$',\n",
    "                 ['Enum_A', 'Enum_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d9d3035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume + year(s)\n",
    "fill_and_extract(r'^' + vvv_vvRE + ' +\\(?(' + yyyy_yyRE + r')\\)?$',\n",
    "                 ['Enum_A', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume + part(s)\n",
    "fill_and_extract(r'^' + vvvRE + r' pt\\. ?(\\d+[a-z]?(?:[\\&\\-]\\d+)?)$',\n",
    "    ['Enum_A', 'Enum_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "553a0d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just issue & nothing else\n",
    "fill_and_extract(r'^no\\. ?(\\d+)$',\n",
    "                 ['Enum_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "04ae50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue + date + year\n",
    "fill_and_extract(r'^no\\. ?(\\d+) (' + mmm_ddRE + ') (' + yyyy_yyRE + r')$',\n",
    "                 ['Enum_B', 'Chron_J', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b2cce271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue + year(s)\n",
    "fill_and_extract(r'^no\\. ?(\\d+) (' + yyyy_yyRE + r')$',\n",
    "                 ['Enum_B', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0035cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just part(s) & nothing else\n",
    "fill_and_extract(r'^pt\\. ?(\\d+[a-z]?(?:[\\&\\-]\\d+)?)$',\n",
    "                 ['Enum_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "75e61d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just year/range of years\n",
    "fill_and_extract(r'^(' + yyyy_yyRE + r')$',\n",
    "                 ['Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32c1ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year(s) + volume(s)\n",
    "fill_and_extract(r'^(' + yyyy_yyRE + r') ' + vvv_vvRE + '$',\n",
    "                 ['Chron_I', 'Enum_A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8ae5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year(s) + part(s)\n",
    "fill_and_extract(r'^(' + yyyy_yyRE + r') pt\\. ?(\\d+(?:[\\-\\/]\\d+)?)$',\n",
    "                 ['Chron_I', 'Enum_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4882f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year(s) + \"Index\"\n",
    "fill_and_extract(r'^(' + yyyy_yyRE + r') (' + iiiiRE + r')$',\n",
    "                 ['Chron_I', 'Enum_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e5c9db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year + month/season/date\n",
    "fill_and_extract(r'^(' + yyyyRE + r') (' + mmm_ddRE + r')$',\n",
    "                 ['Chron_I', 'Chron_J'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07d9102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of dates within one calendar year\n",
    "fill_and_extract(r'^(' + mmm_ddRE + r'[\\-\\/]' + mmm_ddRE + '), (' + yyyyRE + ')$',\n",
    "                 ['Chron_J', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "067ff866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of dates and range of years (only capture years)\n",
    "fill_and_extract(r'^' + mmm_ddRE + r'[\\-\\/]' + mmm_ddRE + ', (' + yyyyRE + r'[\\-\\/]\\d\\d(?:\\d\\d)?)$',\n",
    "    ['Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ef821fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date + year\n",
    "fill_and_extract(r'^(' + mmm_ddRE + '),? (' + yyyyRE + ')$',\n",
    "                 ['Chron_J', 'Chron_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "5c53de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIAL CASES: Range of dates spanning across years\n",
    "#   Only capture the year range, not the dates\n",
    "# ex. \"April 1976-February 1980\" ⇒ 1976-1980\n",
    "exp = re.compile(r'^' + mmm_ddRE + ',? (' + yyyyRE + ') ?- ?' + mmm_ddRE + ',? (' + yyyyRE + ')$')\n",
    "for row, years in df['Description'].str.extract(exp, expand=True).dropna().apply('-'.join, axis=1).items():\n",
    "    df.at[row, 'Chron_I'] = years\n",
    "# ex. \"Nov 11-May 16, 1977-1978\"\n",
    "exp = re.compile(r'^' + mmm_ddRE + ' ?- ?' + mmm_ddRE + ' (' + yyyy_SyyRE + ')$')\n",
    "for row, years in df['Description'].str.extract(exp, expand=True).dropna().apply('-'.join, axis=1).items():\n",
    "    df.at[row, 'Chron_I'] = years.replace('/', '-')\n",
    "# ex. \"Nov 11-May 16, 1977-1978\"\n",
    "exp = re.compile(r'^' + mmm_ddRE + ' ?- ?' + mmm_ddRE + ' (' + yyyy_SyyRE + ')$')\n",
    "for row, years in df['Description'].str.extract(exp, expand=True).dropna().apply('-'.join, axis=1).items():\n",
    "    df.at[row, 'Chron_I'] = years.replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "d31d1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIAL CASES: Range of volumes spanning across years\n",
    "# ex. \"v.16-20 1976-1980\"\n",
    "exp = re.compile(r'^' + vvv_vvRE + ' (' + yyyy_yyRE + ')$')\n",
    "for i, field in enumerate(['Enum_A', 'Chron_I']):\n",
    "    for row, x in df['Description'].str.extract(exp, expand=True).dropna()[i].items():\n",
    "        df.at[row, field] = x\n",
    "# ex. \"v.76 Jan 16, 1986-v.80 Dec 1989\"\n",
    "#   Ignore dates, capture vols & years\n",
    "exp = re.compile(r'^' + vvvRE + ' ' + mmm_ddRE + r',? (' + yyyyRE + ') ?- ?' + vvvRE + ' ' + mmm_ddRE + r',? (' + yyyyRE + ')$')\n",
    "for item, field in enumerate(['Enum_A', 'Chron_I']):\n",
    "    for row, x in df['Description'].str.extract(exp, expand=True).dropna()[[item, item + 2]].apply('-'.join, axis=1).items():\n",
    "        df.at[row, field] = x\n",
    "# ex. \"v.43-45 Jun 21-Jan 30, 1928/30\"\n",
    "#   Ignore dates, capture vols & years\n",
    "exp = re.compile(r'^' + vvv_vvRE + ' ' + mmm_ddRE + ' ?- ?' + mmm_ddRE + ',? (' + yyyy_SyyRE + ')$')\n",
    "for i, field in enumerate(['Enum_A', 'Chron_I']):\n",
    "    for row, x in df['Description'].str.extract(exp, expand=True).dropna()[i].items():\n",
    "        df.at[row, field] = x.replace('/', '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d20e9",
   "metadata": {},
   "source": [
    "#### Then once all those replacements are done, pull filled-in records out to a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "6a65f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to hold JUST records that get filled\n",
    "filled = pd.DataFrame()\n",
    "# Populate the new dataframe with any records that now have at least one Enum/Chron field filled\n",
    "filled = df.dropna(subset=EC_fields, thresh=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4f5ea",
   "metadata": {},
   "source": [
    "#### Then apply the changes via the API and log filled items to the Filled CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "7bd134ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = len(filled)\n",
    "c = 0\n",
    "needs_header=not file_exists(filled_csv) # Apparently we're creating the file, so it needs a header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43078204",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this bit to see what got \"filled\" before hitting the API\n",
    "\n",
    "for index, row in filled.fillna('').iterrows():\n",
    "    c += 1\n",
    "    print(c, ' / '.join([row['MMS_ID'], row['Holdings_ID'], row['Item_ID']]),\n",
    "          str(row['Description']),\n",
    "          ' | '.join(x or '' for x in [row['Enum_A'], row['Enum_B'], row['Enum_C'], row['Chron_I'], row['Chron_J']]),\n",
    "          sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e7b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154f6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dbe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(err_log_txt, 'a') as err_log:\n",
    "    for index, row in filled.fillna('').iterrows():\n",
    "        c += 1\n",
    "        r = requests.get(''.join([baseurl,\n",
    "                                  item_query.format(mms_id=str(row['MMS_ID']),\n",
    "                                                    holding_id=str(row['Holdings_ID']),\n",
    "                                                    item_pid=str(row['Item_ID']),\n",
    "                                                    apikey=apikey)]))\n",
    "        rdict = xmltodict.parse(r.text)\n",
    "        if r.status_code == 429:  # Too many requests--daily limit\n",
    "            print()\n",
    "            print('Reached API request limit for today. Stopping execution.')\n",
    "            print()            \n",
    "            ## Drop this record & everything after from \"filled\"\n",
    "            filled = filled.iloc[:c-1]\n",
    "            break\n",
    "        if r.status_code != 200:\n",
    "            e = xmltodict.parse(r._content)\n",
    "            # Log the error\n",
    "            print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                  ' Error FETCHING item ', row['Item_ID'], ': (', r.status_code, ') ',\n",
    "                  e['web_service_result']['errorList']['error']['errorMessage'],\n",
    "                  sep='',\n",
    "                  file=err_log)\n",
    "            # Remove this item from the \"filled\" df\n",
    "            filled = filled.drop([index])\n",
    "            continue\n",
    "        if (c % (records/100) < 1):\n",
    "            print(int(100*c/records), '% complete', sep='')#, end='\\r')\n",
    "            sleep(5)\n",
    "            \n",
    "        # Merge derived values into the retrieved data (rdict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        rdict['item']['item_data']['description'] = str(row['Description'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        rdict['item']['item_data']['enumeration_a'] = str(row['Enum_A'])\n",
    "        rdict['item']['item_data']['enumeration_b'] = str(row['Enum_B'])\n",
    "        rdict['item']['item_data']['enumeration_c'] = str(row['Enum_C'])\n",
    "        rdict['item']['item_data']['chronology_i'] = str(row['Chron_I'])\n",
    "        rdict['item']['item_data']['chronology_j'] = str(row['Chron_J'])\n",
    "        # Set an internal note, if there's an empty one available\n",
    "        if ('Enum/Chron derived from Description' not in rdict['item']['item_data'].values()):\n",
    "            if (not rdict['item']['item_data']['internal_note_1']):\n",
    "                rdict['item']['item_data']['internal_note_1'] = 'Enum/Chron derived from Description'\n",
    "            elif (not rdict['item']['item_data']['internal_note_2']):\n",
    "                rdict['item']['item_data']['internal_note_2'] = 'Enum/Chron derived from Description'\n",
    "            elif (not rdict['item']['item_data']['internal_note_3']):\n",
    "                rdict['item']['item_data']['internal_note_3'] = 'Enum/Chron derived from Description'\n",
    "            else: # Nbd, just log it\n",
    "                print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                      ' No internal note available for item MMS ID ',\n",
    "                      str(row['MMS_ID']), sep=\"\", file=err_log)\n",
    " \n",
    "        # Push the altered record back into Alma\n",
    "        pxml = xmltodict.unparse(rdict)\n",
    "        p = requests.put(''.join([baseurl,\n",
    "                                  item_query.format(mms_id=row['MMS_ID'],\n",
    "                                                    holding_id=row['Holdings_ID'],\n",
    "                                                    item_pid=row['Item_ID'],\n",
    "                                                    apikey=apikey)]),\n",
    "                         data=pxml.encode('utf-8'), headers={'Content-Type': 'application/xml'})\n",
    "        if r.status_code == 429:  # Too many requests--daily limit\n",
    "            print()\n",
    "            print('Reached API request limit for today. Stopping execution.')\n",
    "            print()            \n",
    "            ## Drop this record & everything after from \"filled\"\n",
    "            filled = filled.iloc[:c-1]\n",
    "            break\n",
    "        if p.status_code != 200:\n",
    "            e = xmltodict.parse(p._content)\n",
    "            # Log the error\n",
    "            print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), ' Error UPDATING item ', row['Item_ID'], ': (', p.status_code, ') ',\n",
    "                e['web_service_result']['errorList']['error']['errorMessage'],\n",
    "                sep='', file=err_log)\n",
    "            # Remove this item from the \"filled\" df\n",
    "            filled = filled.drop([index])\n",
    "            continue\n",
    "        print(c, ' / '.join([row['MMS_ID'], row['Holdings_ID'], row['Item_ID']]),\n",
    "              row['Description'],\n",
    "              ' | '.join(x or '' for x in [row['Enum_A'], row['Enum_B'], row['Chron_I'], row['Chron_J']]),\n",
    "              sep=\"\\t\")\n",
    "\n",
    "        # Log it to the CSV\n",
    "        #    Btw, the 'to_frame().T' transposes it, so it all goes in as a single comma-separated row\n",
    "        row.to_frame().T.to_csv(filled_csv, mode='a', index=False, header=needs_header)\n",
    "        needs_header = False # Henceforth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c578937",
   "metadata": {},
   "source": [
    "#### Purge filled rows from the original CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d347d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purge filled records from the original df\n",
    "df = df.loc[~df['Item_ID'].isin(filled['Item_ID'])]\n",
    "\n",
    "# Re-create the original CSV from that df\n",
    "df.to_csv(exported_csv, index=False) # By default, will overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c2873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb917149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4692307",
   "metadata": {},
   "source": [
    "# View df & filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "display(filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38964dce",
   "metadata": {},
   "source": [
    "# <span style=\"color:#cc0000\">Undo</span> changes to some records!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb425afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "undo_csv=\"Undo.csv\"\n",
    "df = pd.read_csv(undo_csv, converters={'Item_ID': str, 'Holdings_ID': str, 'MMS_ID': str, 'Item ID': str, 'Holdings ID': str, 'MMS ID': str, 'Enum_A': str, 'Enum_B': str, 'Chron_I': str, 'Chron_J': str})\n",
    "# Remove spaces from column names\n",
    "df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "# Rename certain columns\n",
    "df = df.rename(columns={'Permanent_Location': 'Location', 'Item_Policy': 'Policy', 'Material_Type': 'Material'})\n",
    "records = len(df)\n",
    "c = 0\n",
    "\n",
    "for index, row in df.fillna('').iterrows():\n",
    "    c += 1\n",
    "    \n",
    "    # Get the current record\n",
    "    print(c, ' / '.join((row['MMS_ID'], row['Holdings_ID'], row['Item_ID'])), str(row['Description']), sep=\"\\t\")\n",
    "    r = requests.get(''.join([baseurl,\n",
    "                              item_query.format(mms_id=str(row['MMS_ID']),\n",
    "                                                holding_id=str(row['Holdings_ID']),\n",
    "                                                item_pid=str(row['Item_ID']),\n",
    "                                                apikey=apikey)]))\n",
    "    rdict = xmltodict.parse(r.text)\n",
    "    if r.status_code != 200:\n",
    "        e = xmltodict.parse(r._content)\n",
    "        # Output the error\n",
    "        print('Error FETCHING item ', row['Item_ID'], ': (', r.status_code, ') ',\n",
    "            e['web_service_result']['errorList']['error']['errorMessage'],\n",
    "            sep='')\n",
    "        continue\n",
    "    if (c % (records/100) < 1):\n",
    "        print(int(100*c/records), '% complete', sep='')#, end='\\r')\n",
    "        sleep(5)\n",
    "\n",
    "    # Merge derived values into the retrieved data (rdict)\n",
    "    rdict['item']['item_data']['enumeration_a'] = \\\n",
    "        rdict['item']['item_data']['enumeration_b'] = \\\n",
    "        rdict['item']['item_data']['enumeration_c'] = \\\n",
    "        rdict['item']['item_data']['chronology_i'] = \\\n",
    "        rdict['item']['item_data']['chronology_j'] = None\n",
    "    # Set an internal note, if there's an empty one available\n",
    "    if (rdict['item']['item_data']['internal_note_1'] == 'Enum/Chron derived from Description'):\n",
    "        rdict['item']['item_data']['internal_note_1'] = None\n",
    "    if (rdict['item']['item_data']['internal_note_2'] == 'Enum/Chron derived from Description'):\n",
    "        rdict['item']['item_data']['internal_note_2'] = None\n",
    "    if (rdict['item']['item_data']['internal_note_3'] == 'Enum/Chron derived from Description'):\n",
    "        rdict['item']['item_data']['internal_note_3'] = None\n",
    "\n",
    "    # Push the altered record back into Alma\n",
    "    pxml = xmltodict.unparse(rdict)\n",
    "    p = requests.put(''.join([baseurl,\n",
    "                              item_query.format(mms_id=row['MMS_ID'],\n",
    "                                                holding_id=row['Holdings_ID'],\n",
    "                                                item_pid=row['Item_ID'],\n",
    "                                                apikey=apikey)]),\n",
    "                     data=pxml.encode('utf-8'), headers={'Content-Type': 'application/xml'})\n",
    "    if p.status_code != 200:\n",
    "        e = xmltodict.parse(p._content)\n",
    "        # Log the error\n",
    "        print('Error UPDATING item ', row['Item_ID'], ': (', p.status_code, ') ',\n",
    "            e['web_service_result']['errorList']['error']['errorMessage'],\n",
    "            sep='')\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92769b4f",
   "metadata": {},
   "source": [
    "# Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "244feb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?:v|bk)\\. ?(\\d+[a-z]?(?:[\\&\\-]\\d+)?)\n",
      "(?:v|bk)\\. ?(\\d+[a-z]?(?:[\\&\\-]\\d+)?)\n"
     ]
    }
   ],
   "source": [
    "foo = vvvRE[:-1] + r'(?:[\\&\\-]\\d+)?)'\n",
    "print(foo)\n",
    "print(vvv_vvRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "46caef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPECIAL CASE: 3-digit Description, which is a year with first digit removed\n",
    "\n",
    "exp = re.compile(r'^(9\\d{2}(?:\\-19\\d{2}|\\-20\\d{2})?)$')\n",
    "df['Chron_I'] = df['Description'] = '1' + df['Description'].str.extract(exp, expand=True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = re.compile(r'^' + vvv_vvRE + ' ' + mmm_ddRE + ' ?- ?' + mmm_ddRE + ',? (' + yyyyRE + '[\\-\\/]\\d\\d(?:\\d\\d)?)$')\n",
    "for i, field in enumerate(['Enum_A', 'Chron_I']):\n",
    "    for row, x in df['Description'].str.extract(exp, expand=True).dropna()[i].items():\n",
    "#         df.at[row, field] = x.replace('/', '-')\n",
    "        print(row, field, x.replace('/', '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ccc080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Chron_I'] = df['Description'] = '1' + df['Description'].str.extract(exp, expand=True).fillna(df['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1822b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7bab2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City 1    New_York\n",
      "City 2      Lisbon\n",
      "City 3       Tokyo\n",
      "City 4       Paris\n",
      "City 5      Munich\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# importing pandas as pd\n",
    "import pandas as pd\n",
    "  \n",
    "# importing re for regular expressions\n",
    "import re\n",
    "  \n",
    "# Creating the Series\n",
    "sr = pd.Series(['New_York', 'Lisbon', 'Tokyo', 'Paris', 'Munich'])\n",
    "  \n",
    "# Creating the index\n",
    "idx = ['City 1', 'City 2', 'City 3', 'City 4', 'City 5']\n",
    "  \n",
    "# set the index\n",
    "sr.index = idx\n",
    "  \n",
    "# Print the series\n",
    "print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "86e82352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City 1    ew Y\n",
       "City 2      is\n",
       "City 3      ok\n",
       "City 4      ar\n",
       "City 5      un\n",
       "dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr.str.extract('([aeiou].)(?:(_)(.))?').fillna('').replace('_', ' ').apply(''.join, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
